{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af141cc2-1f93-402e-aa87-ae8d183f4305",
   "metadata": {},
   "source": [
    "\"Zero-shot\" in Zero-Shot Image Classification means the model can classify images into categories it has never seen labeled examples of during training.\n",
    "\n",
    "The model learns to connect images with text descriptions rather than training on specific labeled image categories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46715510-6d2e-4337-9d85-3a7380927e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7163f495-134e-4f1d-ba81-f595de00f5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "05018816-99e8-458c-a37c-4e6db2f25fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"data/bike.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "55e324f1-40a8-42f5-adb0-91a6521f3611",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(text=[\"a bike on a street\", \"a car on a street\"], images=image, return_tensors=\"pt\", padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6562b523-7ce1-4d09-ba63-787eb681b63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "98f32820-2155-4164-ad49-efa40c146f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_per_image = outputs.logits_per_image # this is the image-text similarity score\n",
    "probs = logits_per_image.softmax(dim=1) # we can take the softmax to get the label probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "68405ed8-edcf-47c5-a832-f08e73c3802e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9701, 0.0299]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cde9c92-dc4f-4c09-9d96-b2a9ddb1dfa8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
