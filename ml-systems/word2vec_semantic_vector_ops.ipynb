{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b02af2a1-eeed-4dda-a202-84e3325077ec",
   "metadata": {},
   "source": [
    "Pre-trained vectors trained on a part of the Google News dataset (about 100 billion words). The model contains 300-dimensional vectors for 3 million words and phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccc802d6-4ce3-4ebf-b017-d61f92ac5c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html\n",
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "69eb2646-a862-4483-8c92-1aaac69dfd2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cat', 0.8430671691894531), ('baby', 0.8285756707191467), ('kitten', 0.7884738445281982), ('puppy', 0.7541406750679016), ('pup', 0.7308820486068726)]\n"
     ]
    }
   ],
   "source": [
    "closest_words = wv.similar_by_vector(res, topn=5)\n",
    "print(closest_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "17295a6b-7eac-40dc-b1da-8be5f63765f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar_cosmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpositive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnegative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtopn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mrestrict_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Find the top-N most similar words, using the multiplicative combination objective,\n",
       "proposed by `Omer Levy and Yoav Goldberg \"Linguistic Regularities in Sparse and Explicit Word Representations\"\n",
       "<http://www.aclweb.org/anthology/W14-1618>`_. Positive words still contribute positively towards the similarity,\n",
       "negative words negatively, but with less susceptibility to one large distance dominating the calculation.\n",
       "In the common analogy-solving case, of two positive and one negative examples,\n",
       "this method is equivalent to the \"3CosMul\" objective (equation (4)) of Levy and Goldberg.\n",
       "\n",
       "Additional positive or negative examples contribute to the numerator or denominator,\n",
       "respectively - a potentially sensible but untested extension of the method.\n",
       "With a single positive example, rankings will be the same as in the default\n",
       ":meth:`~gensim.models.keyedvectors.KeyedVectors.most_similar`.\n",
       "\n",
       "Allows calls like most_similar_cosmul('dog', 'cat'), as a shorthand for\n",
       "most_similar_cosmul(['dog'], ['cat']) where 'dog' is positive and 'cat' negative\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "positive : list of str, optional\n",
       "    List of words that contribute positively.\n",
       "negative : list of str, optional\n",
       "    List of words that contribute negatively.\n",
       "topn : int or None, optional\n",
       "    Number of top-N similar words to return, when `topn` is int. When `topn` is None,\n",
       "    then similarities for all words are returned.\n",
       "restrict_vocab : int or None, optional\n",
       "    Optional integer which limits the range of vectors which are searched for most-similar values.\n",
       "    For example, restrict_vocab=10000 would only check the first 10000 node vectors in the vocabulary order.\n",
       "    This may be meaningful if vocabulary is sorted by descending frequency.\n",
       "\n",
       "\n",
       "Returns\n",
       "-------\n",
       "list of (str, float) or numpy.array\n",
       "    When `topn` is int, a sequence of (word, similarity) is returned.\n",
       "    When `topn` is None, then similarities for all words are returned as a\n",
       "    one-dimensional numpy array with the size of the vocabulary.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/Projects/ai/.venv/lib/python3.10/site-packages/gensim/models/keyedvectors.py\n",
       "\u001b[0;31mType:\u001b[0m      method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?wv.most_similar_cosmul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f43aae0a-776e-4e74-9e41-10de28b62cf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.9314123392105103),\n",
       " ('monarch', 0.858533501625061),\n",
       " ('princess', 0.8476566672325134),\n",
       " ('Queen_Consort', 0.8150269985198975),\n",
       " ('queens', 0.8099815249443054),\n",
       " ('crown_prince', 0.808997631072998),\n",
       " ('royal_palace', 0.8027306795120239),\n",
       " ('monarchy', 0.8019613027572632),\n",
       " ('prince', 0.800979733467102),\n",
       " ('empress', 0.7958388328552246)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar_cosmul(positive=['king', 'woman'], negative=['man']) # try \"woman\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ef79abe0-884e-4fb0-856d-bba3e490e2d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('blouse', 0.9350481629371643),\n",
       " ('scarf', 0.8647228479385376),\n",
       " ('T_shirt', 0.8582283854484558),\n",
       " ('sleeveless_blouse', 0.8581035733222961),\n",
       " ('bra', 0.8560400605201721),\n",
       " ('sweater', 0.8505845069885254),\n",
       " ('Herve_Leger_dress', 0.8483014702796936),\n",
       " ('floral_blouse', 0.8475623726844788),\n",
       " ('spaghetti_strap_dress', 0.8438350558280945),\n",
       " ('shirts', 0.8410544395446777)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar_cosmul(positive=['shirt', 'woman'], negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "95f0eb94-c1c6-46fc-a97b-d2f5627bf552",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = wv['cat'] + wv['baby']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec00f322-0fdf-45a1-8612-74e8831e2a1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
