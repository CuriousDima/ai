{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "349a3b70-843d-4f6d-9cba-efcbf2d7d208",
      "metadata": {
        "id": "349a3b70-843d-4f6d-9cba-efcbf2d7d208"
      },
      "source": [
        "# Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68fd853b-19af-4906-baeb-c8631d4c9b88",
      "metadata": {
        "id": "68fd853b-19af-4906-baeb-c8631d4c9b88"
      },
      "source": [
        "- [HuggingFase Datasets](https://huggingface.co/docs/datasets) is a library for easily accessing and sharing datasets.\n",
        "- [HuggingFace Tokenizers](https://huggingface.co/docs/tokenizers) is an implementation of today's most used tokenizers, with a focus on performance and versatility."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "591fc34c-211f-47e3-a33a-b945565ae581",
      "metadata": {
        "id": "591fc34c-211f-47e3-a33a-b945565ae581"
      },
      "source": [
        "# Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "afc8e77b-6d83-44a3-949f-969c82543617",
      "metadata": {
        "id": "afc8e77b-6d83-44a3-949f-969c82543617"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1dfd5fe0-2ff5-4ca1-8ee4-66210d9e7585",
      "metadata": {
        "id": "1dfd5fe0-2ff5-4ca1-8ee4-66210d9e7585"
      },
      "outputs": [],
      "source": [
        "# Comp: https://www.kaggle.com/competitions/tweet-sentiment-extraction/overview\n",
        "dataset = load_dataset(\"mteb/tweet_sentiment_extraction\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "076bc917-3cab-4be7-b8dc-ad57a9cde4b2",
      "metadata": {
        "id": "076bc917-3cab-4be7-b8dc-ad57a9cde4b2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eaf40138-5aee-4b53-9e48-c5289dde68f5",
      "metadata": {
        "id": "eaf40138-5aee-4b53-9e48-c5289dde68f5"
      },
      "outputs": [],
      "source": [
        "df_train = pd.DataFrame({\n",
        "    \"text\": dataset[\"train\"][\"text\"],\n",
        "    \"label\": dataset[\"train\"][\"label\"]\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8ca87d0-8203-4de7-bda7-a9c1ac44abff",
      "metadata": {
        "id": "f8ca87d0-8203-4de7-bda7-a9c1ac44abff"
      },
      "outputs": [],
      "source": [
        "df_test = pd.DataFrame({\n",
        "    \"text\": dataset[\"test\"][\"text\"],\n",
        "    \"label\": dataset[\"test\"][\"label\"]\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a99fc94c-d026-4124-802c-d6a197817b57",
      "metadata": {
        "id": "a99fc94c-d026-4124-802c-d6a197817b57"
      },
      "outputs": [],
      "source": [
        "df_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87443e12-de20-432b-9190-a6ff4166aa13",
      "metadata": {
        "id": "87443e12-de20-432b-9190-a6ff4166aa13"
      },
      "outputs": [],
      "source": [
        "df_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3737e5d2-a9d5-48aa-9b01-c1874551c9cc",
      "metadata": {
        "id": "3737e5d2-a9d5-48aa-9b01-c1874551c9cc"
      },
      "outputs": [],
      "source": [
        "df_train[\"label\"].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f63bb0b3-be9f-43bf-a422-19f8b94f2780",
      "metadata": {
        "id": "f63bb0b3-be9f-43bf-a422-19f8b94f2780"
      },
      "source": [
        "# Using Pretrained Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ae127dd-44b5-4287-bf4c-51c772e3154e",
      "metadata": {
        "id": "5ae127dd-44b5-4287-bf4c-51c772e3154e"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd0e647a-aca6-4629-9eb4-860237b1367b",
      "metadata": {
        "id": "bd0e647a-aca6-4629-9eb4-860237b1367b"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "845334de-e709-4539-b470-d9915396f455",
      "metadata": {
        "id": "845334de-e709-4539-b470-d9915396f455",
        "outputId": "49ae9088-4b57-4712-8b9e-7af06a565851"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0, 76269, 304, 342, 13969, 35, 418, 270, 4772, 500, 832, 1225, 588, 15391, 362, 969, 23594, 16, 342, 13969, 35, 344, 295, 1277, 1026, 2656, 5147, 16765, 396, 342, 3518]\n"
          ]
        }
      ],
      "source": [
        "tokens = tokenizer.encode(df_train[\"text\"][5200])\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "257264cc-4467-4273-8f54-9d3dc45b01b5",
      "metadata": {
        "id": "257264cc-4467-4273-8f54-9d3dc45b01b5",
        "outputId": "60e9c116-4cc3-4519-f0ee-3513eb988ab5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<ï½œbeginâ–ofâ–sentenceï½œ>Going to IKEA with the roomie so she can shop for her apartment. IKEA is in like my top ten stores that I love\n"
          ]
        }
      ],
      "source": [
        "orign_text = tokenizer.decode(tokens)\n",
        "print(orign_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8436bc6c-1fe2-4959-b692-6d0822a10e30",
      "metadata": {
        "id": "8436bc6c-1fe2-4959-b692-6d0822a10e30",
        "outputId": "e5359d32-ca15-4f49-cb70-0a0acb716691"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "128815\n"
          ]
        }
      ],
      "source": [
        "print(len(tokenizer.vocab))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12b0e943-4053-4159-9030-6c41dc571892",
      "metadata": {
        "id": "12b0e943-4053-4159-9030-6c41dc571892",
        "outputId": "8ada266e-0008-4b1c-d496-75fab647ead0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[     1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
            "              0,  53091,   4374,   1465,    442,     85,  17840,   9160,   2984,\n",
            "           1585,    377,   3947,     16],\n",
            "        [     0,  83469,  17840,   1878,    344,  28179,    418, 112838,    248,\n",
            "          38178,    387,     16,   2275,    270,   3418,   2231,    366,   1606,\n",
            "            304,   1801,   5671,  16496]])\n",
            "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
          ]
        }
      ],
      "source": [
        "texts = [\n",
        "    \"DeepSeekâ€™s tokenizer works well on English.\",\n",
        "    \"Batch tokenization is straightforward with ðŸ¤— Transformers. But the string must be long to show attention mask\"\n",
        "]\n",
        "\n",
        "# Batch processing\n",
        "batch = tokenizer(\n",
        "    texts,\n",
        "    padding=True,        # pad to the longest sequence in the batch\n",
        "    truncation=True,     # truncate sequences that exceed modelâ€™s max length\n",
        "    max_length=256,      # optional: set an explicit limit\n",
        "    return_tensors=\"pt\"  # return PyTorch tensors (use \"tf\" for TensorFlow)\n",
        ")\n",
        "\n",
        "print(batch[\"input_ids\"])      # tensor of token IDs\n",
        "print(batch[\"attention_mask\"]) # tensor indicating which tokens are padding"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edec7c6e-043e-432d-9d86-7e2e07bdc84d",
      "metadata": {
        "id": "edec7c6e-043e-432d-9d86-7e2e07bdc84d"
      },
      "source": [
        "# Training a Text Classifier (FastAI)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "da7cd0a3-cbdb-40a2-a5fc-a038c344f64e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "da7cd0a3-cbdb-40a2-a5fc-a038c344f64e",
        "outputId": "2a1d48a9-bfe6-4a0e-c28c-80a61951367e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      <progress value='144441344' class='' max='144440600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      100.00% [144441344/144440600 00:02&lt;00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Download an IMDB dataset\n",
        "from fastai.text.all import *\n",
        "path = untar_data(URLs.IMDB)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "cafa0f21-86bd-4c1c-a13f-d4ad5be09c19",
      "metadata": {
        "id": "cafa0f21-86bd-4c1c-a13f-d4ad5be09c19"
      },
      "outputs": [],
      "source": [
        "get_imdb = partial(get_text_files, folders=['train', 'test', 'unsup'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "6d97dad3-0e68-4418-8b6e-20d2c41f586c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "6d97dad3-0e68-4418-8b6e-20d2c41f586c",
        "outputId": "0ddf7c49-84b7-4aa4-8d84-773b333b37d9"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        }
      ],
      "source": [
        "datablock = DataBlock(\n",
        "    blocks=TextBlock.from_folder(path, is_lm=True),\n",
        "    get_items=get_imdb, splitter=RandomSplitter(0.1)\n",
        ")\n",
        "dls_lm = datablock.dataloaders(path, path=path, bs=128, seq_len=80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "5fcff1e7-483c-4fae-91e9-76d6a5cdf12a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "5fcff1e7-483c-4fae-91e9-76d6a5cdf12a",
        "outputId": "3cf5a711-2b46-4581-e0e2-db2cd3a93308"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>text_</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>xxbos xxmaj certain elements of this film are dated , of course . xxmaj an all white male crew , for instance . xxmaj and like most pre - star xxmaj wars xxmaj science xxmaj fiction , it tends to take too long admiring itself . \\n\\n xxmaj but , still , no movie has ever capture the flavor of xxmaj golden xxmaj age xxmaj science xxmaj fiction as this one did , even down to the use of the</td>\n",
              "      <td>xxmaj certain elements of this film are dated , of course . xxmaj an all white male crew , for instance . xxmaj and like most pre - star xxmaj wars xxmaj science xxmaj fiction , it tends to take too long admiring itself . \\n\\n xxmaj but , still , no movie has ever capture the flavor of xxmaj golden xxmaj age xxmaj science xxmaj fiction as this one did , even down to the use of the \"</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\" womans xxmaj choice \" ! xxmaj this film will provoke you to reconsider . \\n\\n xxmaj even though most of the actors have only been in a few films , you will wonder why they have not been cast more often . \\n\\n xxmaj if you watch this film and are not challenged by its thought provoking message , you need to watch it again because you did not pay attention the first time . xxbos xxmaj perhaps xxmaj</td>\n",
              "      <td>womans xxmaj choice \" ! xxmaj this film will provoke you to reconsider . \\n\\n xxmaj even though most of the actors have only been in a few films , you will wonder why they have not been cast more often . \\n\\n xxmaj if you watch this film and are not challenged by its thought provoking message , you need to watch it again because you did not pay attention the first time . xxbos xxmaj perhaps xxmaj i</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>in la la land when the rangers jump out of a xxmaj hercules transport at dawn somewhere over the mideast , but then after a water landing they surface in the dark ! xxmaj the continuity errors continue xxunk the pic , costumes and make up change multiple times within scenes . xxmaj but ya know what , it does'nt matter ! xxmaj the script is even more ludicrous . xxmaj after the xxmaj rangers capture a terrorist and bring</td>\n",
              "      <td>la la land when the rangers jump out of a xxmaj hercules transport at dawn somewhere over the mideast , but then after a water landing they surface in the dark ! xxmaj the continuity errors continue xxunk the pic , costumes and make up change multiple times within scenes . xxmaj but ya know what , it does'nt matter ! xxmaj the script is even more ludicrous . xxmaj after the xxmaj rangers capture a terrorist and bring him</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Create a dataset for training. Dependent variable is one token ahead of a dependent variable\n",
        "dls_lm.show_batch(max_n=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "2bf503c7-0e3b-4c25-ad3f-cf0b4f6256a9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "2bf503c7-0e3b-4c25-ad3f-cf0b4f6256a9",
        "outputId": "21b39997-e603-466c-d1c6-02e1b3e56b09"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      <progress value='105070592' class='' max='105067061' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      100.00% [105070592/105067061 00:01&lt;00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "learn = language_model_learner(\n",
        "    dls_lm, AWD_LSTM, drop_mult=0.3,\n",
        "    metrics=[accuracy, Perplexity()]).to_fp16()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "148ae881-a536-44e7-9a40-c748f61e67cc",
      "metadata": {
        "id": "148ae881-a536-44e7-9a40-c748f61e67cc"
      },
      "source": [
        "The loss function used by default is cross-entropy loss, since we essentially have a classification problem (the different categories being the words in our vocab)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57918a4d-98d8-4eb8-a7f5-b47d252eebf9",
      "metadata": {
        "id": "57918a4d-98d8-4eb8-a7f5-b47d252eebf9"
      },
      "source": [
        "![Alt Text](https://raw.githubusercontent.com/fastai/fastbook/e8baa81d89f0b7be816e35f1cc813ac02038db54/images/att_00027.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91402306-dbb2-48d5-9f3d-a627ff0d5446",
      "metadata": {
        "id": "91402306-dbb2-48d5-9f3d-a627ff0d5446"
      },
      "source": [
        "The first arrow has been completed for us and made available as a pretrained model in fastai, and we've just built the DataLoaders and Learner for the second stage. Now we're ready to fine-tune our language model!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c28b053d-53be-4240-8204-dfc91a3844b3",
      "metadata": {
        "id": "c28b053d-53be-4240-8204-dfc91a3844b3"
      },
      "source": [
        "`language_model_learner` automatically calls `freeze` when using a pretrained model (which is the default), so this will only train the embeddings (the only part of the model that contains randomly initialized weightsâ€”i.e., embeddings for words that are in our IMDb vocab, but aren't in the pretrained model vocab):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "8a1054f9-e484-42be-bf54-aba498dca242",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172
        },
        "id": "8a1054f9-e484-42be-bf54-aba498dca242",
        "outputId": "85578875-25b7-45cf-ece7-a508ee861769"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n",
            "/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>perplexity</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>4.007000</td>\n",
              "      <td>3.896233</td>\n",
              "      <td>0.300810</td>\n",
              "      <td>49.216682</td>\n",
              "      <td>09:49</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "learn.fit_one_cycle(1, 2e-2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db5f161c-61b4-4e96-9b81-ba19183a310f",
      "metadata": {
        "id": "db5f161c-61b4-4e96-9b81-ba19183a310f"
      },
      "source": [
        "Once the initial training has completed, we can continue fine-tuning the model after unfreezing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "f9c8182c-feca-443b-8a41-67addbc0774c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "f9c8182c-feca-443b-8a41-67addbc0774c",
        "outputId": "8c1a7854-17ae-4a23-e9ba-a5f6d477a6cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n",
            "/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>perplexity</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>3.761386</td>\n",
              "      <td>3.750302</td>\n",
              "      <td>0.318114</td>\n",
              "      <td>42.533920</td>\n",
              "      <td>10:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.675211</td>\n",
              "      <td>3.656346</td>\n",
              "      <td>0.328566</td>\n",
              "      <td>38.719604</td>\n",
              "      <td>09:52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.562186</td>\n",
              "      <td>3.599485</td>\n",
              "      <td>0.335250</td>\n",
              "      <td>36.579395</td>\n",
              "      <td>09:54</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.447156</td>\n",
              "      <td>3.568553</td>\n",
              "      <td>0.339424</td>\n",
              "      <td>35.465252</td>\n",
              "      <td>09:42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>3.365878</td>\n",
              "      <td>3.568156</td>\n",
              "      <td>0.340122</td>\n",
              "      <td>35.451153</td>\n",
              "      <td>09:34</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "learn.unfreeze()\n",
        "learn.fit_one_cycle(5, 2e-3)  # \"fit_one_cycle\" allows to save a model after each epoch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "660d931d-29d2-43d0-a1b8-fe34cac57634",
      "metadata": {
        "id": "660d931d-29d2-43d0-a1b8-fe34cac57634"
      },
      "source": [
        "Once this is done, we save all of our model except the final layer that converts activations to probabilities of picking each token in our vocabulary. The model not including the final layer is called the encoder. We can save it with `save_encoder`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "49abf6f9-d117-4cd5-99fa-a9b36ef1444d",
      "metadata": {
        "id": "49abf6f9-d117-4cd5-99fa-a9b36ef1444d"
      },
      "outputs": [],
      "source": [
        "learn.save('finetuned')\n",
        "learn.save_encoder('finetuned_encoder')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e30fae6-932b-421b-8d2e-8f0eb1d051ab",
      "metadata": {
        "id": "8e30fae6-932b-421b-8d2e-8f0eb1d051ab"
      },
      "outputs": [],
      "source": [
        "# We can check how we did with Text Generation:\n",
        "TEXT = \"I liked this movie because\"\n",
        "N_WORDS = 40\n",
        "N_SENTENCES = 2\n",
        "preds = [learn.predict(TEXT, N_WORDS, temperature=0.75)\n",
        "         for _ in range(N_SENTENCES)]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\".join(preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_5tzGv7DnCE",
        "outputId": "600199a0-ab62-4c5f-c69f-1a544e29b35b"
      },
      "id": "p_5tzGv7DnCE",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i liked this movie because it has one of the best performances I 've seen in a long time . It 's a good movie to see if you want to get your heart pumping , or if you 're a fan of\n",
            "i liked this movie because of the fact that it was longer . It 's about a teenager , and does n't realize what he 's doing . This movie plays on with the spirit of the father . It 's not\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "g2SP1OBpyw4b",
      "metadata": {
        "id": "g2SP1OBpyw4b"
      },
      "source": [
        "# Classifier Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "vlh1TlMv3fjB",
      "metadata": {
        "id": "vlh1TlMv3fjB"
      },
      "outputs": [],
      "source": [
        "dls_clas = DataBlock(\n",
        "    blocks=(TextBlock.from_folder(path, vocab=dls_lm.vocab),CategoryBlock),\n",
        "    get_y = parent_label,\n",
        "    get_items=partial(get_text_files, folders=['train', 'test']),\n",
        "    splitter=GrandparentSplitter(valid_name='test')\n",
        ").dataloaders(path, path=path, bs=128, seq_len=72)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dls_clas.show_batch(max_n=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "-REV1C1UH6JV",
        "outputId": "6520ec23-186d-439d-f46b-9f1a3fee5597"
      },
      "id": "-REV1C1UH6JV",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>xxbos xxmaj match 1 : xxmaj tag xxmaj team xxmaj table xxmaj match xxmaj bubba xxmaj ray and xxmaj spike xxmaj dudley vs xxmaj eddie xxmaj guerrero and xxmaj chris xxmaj benoit xxmaj bubba xxmaj ray and xxmaj spike xxmaj dudley started things off with a xxmaj tag xxmaj team xxmaj table xxmaj match against xxmaj eddie xxmaj guerrero and xxmaj chris xxmaj benoit . xxmaj according to the rules of the match , both opponents have to go through tables in order to get the win . xxmaj benoit and xxmaj guerrero heated up early on by taking turns hammering first xxmaj spike and then xxmaj bubba xxmaj ray . a xxmaj german xxunk by xxmaj benoit to xxmaj bubba took the wind out of the xxmaj dudley brother . xxmaj spike tried to help his brother , but the referee restrained him while xxmaj benoit and xxmaj guerrero</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>xxbos xxmaj by now you 've probably heard a bit about the new xxmaj disney dub of xxmaj miyazaki 's classic film , xxmaj laputa : xxmaj castle xxmaj in xxmaj the xxmaj sky . xxmaj during late summer of 1998 , xxmaj disney released \" kiki 's xxmaj delivery xxmaj service \" on video which included a preview of the xxmaj laputa dub saying it was due out in \" 1 xxrep 3 9 \" . xxmaj it 's obviously way past that year now , but the dub has been finally completed . xxmaj and it 's not \" laputa : xxmaj castle xxmaj in xxmaj the xxmaj sky \" , just \" castle xxmaj in xxmaj the xxmaj sky \" for the dub , since xxmaj laputa is not such a nice word in xxmaj spanish ( even though they use the word xxmaj laputa many times</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>xxbos xxmaj some have praised _ xxunk _ as a xxmaj disney adventure for adults . i do n't think so -- at least not for thinking adults . \\n\\n xxmaj this script suggests a beginning as a live - action movie , that struck someone as the type of crap you can not sell to adults anymore . xxmaj the \" crack staff \" of many older adventure movies has been done well before , ( think _ the xxmaj dirty xxmaj dozen _ ) but _ atlantis _ represents one of the worse films in that motif . xxmaj the characters are weak . xxmaj even the background that each member trots out seems stock and awkward at best . xxmaj an xxup md / xxmaj medicine xxmaj man , a tomboy mechanic whose father always wanted sons , if we have not at least seen these before</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learn = text_classifier_learner(dls_clas, AWD_LSTM, drop_mult=0.5,\n",
        "                                metrics=accuracy).to_fp16()"
      ],
      "metadata": {
        "id": "jKOVfDN4ImBT"
      },
      "id": "jKOVfDN4ImBT",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can now load encoder since we are going to add a classification head.\n",
        "learn = learn.load_encoder('/content/finetuned_encoder')"
      ],
      "metadata": {
        "id": "DMdWxeoPJ-Yd"
      },
      "id": "DMdWxeoPJ-Yd",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-Tuning the Classifier"
      ],
      "metadata": {
        "id": "d0uflOB7K7JC"
      },
      "id": "d0uflOB7K7JC"
    },
    {
      "cell_type": "code",
      "source": [
        "learn.fit_one_cycle(1, 2e-2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172
        },
        "id": "xzOEpwszK_cN",
        "outputId": "195b62dd-0a1a-4cca-8865-b8114409593b"
      },
      "id": "xzOEpwszK_cN",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n",
            "/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.294827</td>\n",
              "      <td>0.224858</td>\n",
              "      <td>0.910720</td>\n",
              "      <td>00:15</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -2 - freeze all except the last two parameters.\n",
        "learn.freeze_to(-2)\n",
        "learn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172
        },
        "id": "McXiA4BsLWII",
        "outputId": "ee89f00d-ffcc-404e-e704-0307b2e5b57f"
      },
      "id": "McXiA4BsLWII",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n",
            "/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.262539</td>\n",
              "      <td>0.206517</td>\n",
              "      <td>0.918640</td>\n",
              "      <td>00:18</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learn.freeze_to(-3)\n",
        "learn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172
        },
        "id": "G1r-0ya0MSed",
        "outputId": "fc0d4547-ed74-469a-bf27-2044fe34871b"
      },
      "id": "G1r-0ya0MSed",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n",
            "/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.228580</td>\n",
              "      <td>0.181586</td>\n",
              "      <td>0.930960</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learn.unfreeze()\n",
        "learn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "GwWTcBlMMWJU",
        "outputId": "4bbc03e7-dba0-47a9-f3f4-288373fe6047"
      },
      "id": "GwWTcBlMMWJU",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n",
            "/usr/local/lib/python3.11/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.197066</td>\n",
              "      <td>0.173745</td>\n",
              "      <td>0.933360</td>\n",
              "      <td>00:24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.175426</td>\n",
              "      <td>0.173970</td>\n",
              "      <td>0.935160</td>\n",
              "      <td>00:24</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s0N8NHQwVDPR"
      },
      "id": "s0N8NHQwVDPR",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}